{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHRkE30vcr97",
        "outputId": "ddceb1d2-ffb9-4de7-db2a-72436bb310c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "# Fetch the Boston housing dataset\n",
        "boston = fetch_openml(name='boston', version=1)\n",
        "\n",
        "# Convert to DataFrame\n",
        "boston_df = pd.DataFrame(data=np.c_[boston['data'], boston['target']],\n",
        "                         columns=list(boston['feature_names']) + ['MEDV'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming we want to classify as \"high price\" (1) or \"low price\" (0) based on median\n",
        "median_price = boston_df['MEDV'].median()\n",
        "boston_df['price_class'] = (boston_df['MEDV'] > median_price).astype(int)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X = boston_df.drop(['MEDV', 'price_class'], axis=1)\n",
        "y = boston_df['price_class']\n",
        "\n",
        "# Ensure proper type and values for y\n",
        "print(f\"Unique values in y: {y.unique()}\")\n",
        "print(f\"Data type of y: {y.dtype}\")\n",
        "\n",
        "# Convert y to numpy array if it's not already\n",
        "y = np.array(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nR78FQKTgMnh",
        "outputId": "7150922f-d67c-435b-b8af-27d2934ebb2b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in y: [1 0]\n",
            "Data type of y: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Data preprocessing (scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "tBC3_-RReVqL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model (Logistic Regression)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "\n",
        "# Print classification report\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(f\"Classification Report:\\n{class_report}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcABM_xfeXIO",
        "outputId": "1b9118fc-2bc8-4161-85ce-22d1ca026709"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8725490196078431\n",
            "Confusion Matrix:\n",
            "[[52  8]\n",
            " [ 5 37]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.87      0.89        60\n",
            "           1       0.82      0.88      0.85        42\n",
            "\n",
            "    accuracy                           0.87       102\n",
            "   macro avg       0.87      0.87      0.87       102\n",
            "weighted avg       0.88      0.87      0.87       102\n",
            "\n"
          ]
        }
      ]
    }
  ]
}