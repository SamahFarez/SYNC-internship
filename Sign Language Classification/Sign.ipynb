{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b5ikN8Vievo",
        "outputId": "9cad86b5-12cd-4402-f9d2-0e79ac4db1ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Sign-Language-Digits-Dataset'...\n",
            "remote: Enumerating objects: 2095, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 2095 (delta 2), reused 0 (delta 0), pack-reused 2089\u001b[K\n",
            "Receiving objects: 100% (2095/2095), 15.07 MiB | 11.59 MiB/s, done.\n",
            "Resolving deltas: 100% (660/660), done.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 62, 62, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 31, 31, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 29, 29, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 14, 14, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 12, 12, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 6, 6, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 4608)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               589952    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 683914 (2.61 MB)\n",
            "Trainable params: 683914 (2.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "52/52 [==============================] - 14s 224ms/step - loss: 2.2875 - accuracy: 0.1498 - val_loss: 2.0933 - val_accuracy: 0.4600\n",
            "Epoch 2/10\n",
            "52/52 [==============================] - 12s 222ms/step - loss: 1.5769 - accuracy: 0.4433 - val_loss: 1.0375 - val_accuracy: 0.6925\n",
            "Epoch 3/10\n",
            "52/52 [==============================] - 12s 226ms/step - loss: 1.0383 - accuracy: 0.6380 - val_loss: 0.6438 - val_accuracy: 0.8087\n",
            "Epoch 4/10\n",
            "52/52 [==============================] - 11s 208ms/step - loss: 0.8201 - accuracy: 0.7186 - val_loss: 0.4745 - val_accuracy: 0.8571\n",
            "Epoch 5/10\n",
            "52/52 [==============================] - 11s 210ms/step - loss: 0.6044 - accuracy: 0.7932 - val_loss: 0.3878 - val_accuracy: 0.8717\n",
            "Epoch 6/10\n",
            "52/52 [==============================] - 12s 229ms/step - loss: 0.5137 - accuracy: 0.8320 - val_loss: 0.2920 - val_accuracy: 0.9104\n",
            "Epoch 7/10\n",
            "52/52 [==============================] - 12s 224ms/step - loss: 0.4498 - accuracy: 0.8532 - val_loss: 0.2664 - val_accuracy: 0.9128\n",
            "Epoch 8/10\n",
            "52/52 [==============================] - 12s 224ms/step - loss: 0.3749 - accuracy: 0.8636 - val_loss: 0.2394 - val_accuracy: 0.9225\n",
            "Epoch 9/10\n",
            "52/52 [==============================] - 11s 215ms/step - loss: 0.3091 - accuracy: 0.9018 - val_loss: 0.2280 - val_accuracy: 0.9298\n",
            "Epoch 10/10\n",
            "52/52 [==============================] - 11s 208ms/step - loss: 0.2860 - accuracy: 0.9012 - val_loss: 0.2491 - val_accuracy: 0.9274\n",
            "13/13 [==============================] - 1s 47ms/step\n",
            "Test Accuracy: 0.9274\n",
            "Confusion Matrix:\n",
            "[[38  0  0  0  0  0  0  0  0  0]\n",
            " [ 0 47  1  0  0  0  0  0  1  0]\n",
            " [ 0  0 44  1  0  0  0  0  0  0]\n",
            " [ 0  0  0 32  0  0  0  0  0  0]\n",
            " [ 0  0  1  1 48  0  1  0  1  0]\n",
            " [ 0  0  0  0  0 40  0  0  0  0]\n",
            " [ 0  0  6  0  2  0 30  2  0  0]\n",
            " [ 0  0  6  0  3  0  0 37  0  0]\n",
            " [ 0  0  0  0  1  0  0  0 37  1]\n",
            " [ 0  0  1  0  0  0  0  0  1 30]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        38\n",
            "           1       1.00      0.96      0.98        49\n",
            "           2       0.75      0.98      0.85        45\n",
            "           3       0.94      1.00      0.97        32\n",
            "           4       0.89      0.92      0.91        52\n",
            "           5       1.00      1.00      1.00        40\n",
            "           6       0.97      0.75      0.85        40\n",
            "           7       0.95      0.80      0.87        46\n",
            "           8       0.93      0.95      0.94        39\n",
            "           9       0.97      0.94      0.95        32\n",
            "\n",
            "    accuracy                           0.93       413\n",
            "   macro avg       0.94      0.93      0.93       413\n",
            "weighted avg       0.94      0.93      0.93       413\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Step 1: Clone the Sign Language MNIST dataset from GitHub\n",
        "!git clone https://github.com/ardamavi/Sign-Language-Digits-Dataset.git\n",
        "\n",
        "# Step 2: Load and preprocess the dataset\n",
        "def load_data(dataset_path):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for folder in os.listdir(dataset_path):\n",
        "        label = int(folder)  # Assuming folders are named 0, 1, ..., 9\n",
        "        folder_path = os.path.join(dataset_path, folder)\n",
        "        for filename in os.listdir(folder_path):\n",
        "            image_path = os.path.join(folder_path, filename)\n",
        "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "            image = cv2.resize(image, (64, 64))\n",
        "            images.append(image)\n",
        "            labels.append(label)\n",
        "    images = np.array(images, dtype=np.float32) / 255.0  # Normalize pixel values to [0, 1]\n",
        "    labels = np.array(labels, dtype=np.int32)\n",
        "    return images, labels\n",
        "\n",
        "dataset_path = 'Sign-Language-Digits-Dataset/Dataset'\n",
        "X, y = load_data(dataset_path)\n",
        "\n",
        "# Step 3: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Build the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')  # 10 classes (0-9)\n",
        "])\n",
        "\n",
        "# Step 5: Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Step 6: Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Step 7: Reshape X_train and X_test for grayscale images\n",
        "X_train = X_train.reshape((-1, 64, 64, 1))\n",
        "X_test = X_test.reshape((-1, 64, 64, 1))\n",
        "\n",
        "# Step 8: Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
        "\n",
        "# Step 9: Evaluate the model on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Step 10: Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Step 11: Generate confusion matrix and classification report\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
        "class_report = classification_report(y_test, y_pred_classes)\n",
        "\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "print(f\"Classification Report:\\n{class_report}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 12: Predict on a new image\n",
        "def predict_image(image_path):\n",
        "    # Load and preprocess the image\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    img = cv2.resize(img, (64, 64))\n",
        "    img = np.expand_dims(img, axis=-1)\n",
        "    img = np.expand_dims(img, axis=0) / 255.0\n",
        "\n",
        "    # Predict class probabilities\n",
        "    pred = model.predict(img)\n",
        "    class_idx = np.argmax(pred)\n",
        "\n",
        "    # Map class index to sign language digit (0-9)\n",
        "    sign_digits = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "    predicted_digit = sign_digits[class_idx]\n",
        "\n",
        "    return predicted_digit\n",
        "\n",
        "# Example usage:\n",
        "image_path = 'sign2.jpg'  # Replace with your image path\n",
        "predicted_digit = predict_image(image_path)\n",
        "print(f\"Predicted Sign Language Digit: {predicted_digit}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTn9fKeGigUw",
        "outputId": "28db5c65-17de-415a-a1e9-ee15e75e368c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 38ms/step\n",
            "Predicted Sign Language Digit: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y5l08ILflThA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}